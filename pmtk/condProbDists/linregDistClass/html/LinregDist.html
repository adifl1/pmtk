
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      -->
      <title>LinregDist</title>
      <meta name="generator" content="MATLAB 7.7">
      <meta name="date" content="2008-11-06">
      <meta name="m-file" content="LinregDist"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head>
   <body>
      <div class="content">
         <h2>Contents</h2>
         <div>
            <ul>
               <li><a href="#2">Linear Regression Conditional Distribution</a></li>
               <li><a href="#3">Main methods</a></li>
               <li><a href="#4">Protected Methods</a></li>
            </ul>
         </div><pre class="codeinput"><span class="keyword">classdef</span> LinregDist &lt; condProbDist
</pre><h2>Linear Regression Conditional Distribution<a name="2"></a></h2><pre class="codeinput">    properties
        w;                   <span class="comment">% w is the posterior distribution of the weights.</span>
                             <span class="comment">% The form depends on how this object was fit.</span>
                             <span class="comment">% If method = 'map', the default, then w</span>
                             <span class="comment">% represents the MAP estimate and is stored as a</span>
                             <span class="comment">% constDist object. If method = 'bayesian', then</span>
                             <span class="comment">% the form is conjugate to the prior, i.e. either</span>
                             <span class="comment">% an mvnDist object in the case that the variance,</span>
                             <span class="comment">% sigma2, is known, or an mvnInvGammaDist object in</span>
                             <span class="comment">% the case that both the mean and variance are</span>
                             <span class="comment">% unknown.</span>

        sigma2;              <span class="comment">% Fixed/Known value for the variance in</span>
                             <span class="comment">% p(y | w,X) = N(y | Xw, sigma2*I). This value is</span>
                             <span class="comment">% ignored when an mvnInvGammaDist prior is</span>
                             <span class="comment">% specified.</span>

        transformer;         <span class="comment">% A data transformer object, e.g. kernelTransformer</span>
    <span class="keyword">end</span>
</pre><h2>Main methods<a name="3"></a></h2><pre class="codeinput">    methods
        <span class="keyword">function</span> model = LinregDist(varargin)
        <span class="comment">%Constructor</span>
        <span class="comment">%</span>
        <span class="comment">% FORMAT:</span>
        <span class="comment">%           model = LinRegDist('name1',val1,'name2',val2,...);</span>
        <span class="comment">%</span>
        <span class="comment">%</span>
        <span class="comment">% INPUT:</span>
        <span class="comment">%</span>
        <span class="comment">% 'transformer'     - An optional transformer object such as  kernelTransformer</span>
        <span class="comment">%</span>
        <span class="comment">% 'w'               - The posterior of the weights: either a constDist,</span>
        <span class="comment">%                     mvnDist or mvnInvGammaDist object. This will</span>
        <span class="comment">%                     usually be learned when fitting the object and can</span>
        <span class="comment">%                     be left unspecified. If w is a numeric matrix, it</span>
        <span class="comment">%                     is converted to a constDist object.</span>
        <span class="comment">%</span>
        <span class="comment">% 'sigma2'          - Fixed/Known value for the variance in</span>
        <span class="comment">%                     p(y | w,X) = N(y | Xw, sigma2*I). This is only used</span>
        <span class="comment">%                     when the fit method is 'bayesian' and the specified</span>
        <span class="comment">%                     prior is an mvnDist object. When the object</span>
        <span class="comment">%                     is fit using method = MAP, this is estimated as</span>
        <span class="comment">%                     mean( (yhat-y).^2). This can also be specified</span>
        <span class="comment">%                     when fitting the model.</span>
        <span class="comment">%</span>
        <span class="comment">% OUTPUT:</span>
        <span class="comment">%</span>
        <span class="comment">% model             - The constructed LinregDist object</span>
        <span class="comment">%</span>

            [transformer, w, sigma2] = process_options(varargin,<span class="keyword">...</span>
                        <span class="string">'transformer'</span>, []                      ,<span class="keyword">...</span>
                        <span class="string">'w'</span>          , []                      ,<span class="keyword">...</span>
                        <span class="string">'sigma2'</span>     , []                      );

            m.transformer = transformer;
            <span class="keyword">if</span>(isnumeric(w) &amp;&amp; ~isempty(w))
                model.w = constDist(w);
            <span class="keyword">else</span>
                model.w = w;
            <span class="keyword">end</span>
            model.sigma2 = sigma2;
        <span class="keyword">end</span>


        <span class="keyword">function</span> model = fit(model,varargin)
        <span class="comment">% Compute the posterior distribution over w, the weights. This is either</span>
        <span class="comment">% a constant distribution representing the MAP estimate if method =</span>
        <span class="comment">% 'map', (the default), or conjugate to the specified prior object, i.e.</span>
        <span class="comment">% one of mvnDist or mvnInvGammaDist.</span>
        <span class="comment">%</span>
        <span class="comment">% FORMAT:</span>
        <span class="comment">%           model = fit(model,'name1',val1,'name2',val2,...)</span>
        <span class="comment">%</span>
        <span class="comment">% INPUT</span>
        <span class="comment">%</span>
        <span class="comment">% 'X'      - The training examples: X(i,:) is the ith case</span>
        <span class="comment">%</span>
        <span class="comment">% 'y'      - The output or target values s.t. y(i,:) corresponds to</span>
        <span class="comment">%            X(i,:); y may be multidimensional.</span>
        <span class="comment">%</span>
        <span class="comment">% 'prior'  - In the case that method = 'map' estimation,(default), one</span>
        <span class="comment">%            of four strings can be specified, 'none' | 'L1' | 'L2' | 'L1L2'</span>
        <span class="comment">%            where 'none' corresponds to the mle and 'L1L2' corresponds</span>
        <span class="comment">%            to elastic net.</span>
        <span class="comment">%</span>
        <span class="comment">%            In the case that method = 'bayesian', you can specify a</span>
        <span class="comment">%            preconstructed mvnDist object or an mvnInvGammaDist object.</span>
        <span class="comment">%            Alternatively, you can specify the string 'mvn' or 'mvnIG'</span>
        <span class="comment">%            and an object will be constructed for you. In these latter</span>
        <span class="comment">%            two cases, the prior is spherical with lambda used as</span>
        <span class="comment">%            the precision, (except for the offset terms if any) and</span>
        <span class="comment">%            other values set to reasonable defaults.</span>
        <span class="comment">%</span>
        <span class="comment">% 'lambda'   In the case of map estimation, this is the regularization</span>
        <span class="comment">%            value. In the case of 'bayesian' estimation where the</span>
        <span class="comment">%            'prior' was specified as a string, this acts as the</span>
        <span class="comment">%            precision.</span>
        <span class="comment">%</span>
        <span class="comment">% 'method'   One of ['map' | 'bayesian']</span>
        <span class="comment">%</span>
        <span class="comment">% 'algorithm' Only used when method is 'map'</span>
        <span class="comment">%</span>
        <span class="comment">%            case: {'L1','L1L2'}  ['lars'] | 'shooting'</span>
        <span class="comment">%            case: {'L2'}         ['ridgeQR'] | 'ridgeSVD']</span>
        <span class="comment">%</span>
        <span class="comment">%</span>
            method = process_options(varargin, <span class="string">'method'</span> , <span class="string">'map'</span>);
            <span class="keyword">switch</span> lower(method)

                <span class="keyword">case</span> {<span class="string">'map'</span>,<span class="string">'mle'</span>}
                   model = fitMAP(model,varargin{:});
                <span class="keyword">case</span> <span class="string">'bayesian'</span>
                   model = fitBayesian(model,varargin{:});
                <span class="keyword">otherwise</span>
                    error(<span class="string">'%s is not supported - choose one of ''map'' or ''bayesian'''</span>,method);
            <span class="keyword">end</span>


        <span class="keyword">end</span>

        <span class="keyword">function</span> py = predict(model, varargin)
        <span class="comment">% Return a predictive distribution given the fitted model and the</span>
        <span class="comment">% test set examples, X.</span>
        <span class="comment">%</span>
        <span class="comment">% FORMAT:</span>
        <span class="comment">%</span>
        <span class="comment">% model = predict(model,'name1',val1,'name2',val2,...);</span>
        <span class="comment">%</span>
        <span class="comment">% OR</span>
        <span class="comment">%</span>
        <span class="comment">% model = predict(model,X)</span>
        <span class="comment">%</span>
        <span class="comment">% INPUT:</span>
        <span class="comment">%</span>
        <span class="comment">% 'X'        - The test set examples, X(i,:) is example i</span>
        <span class="comment">%</span>
        <span class="comment">% 'method'   - ['plugin'] | 'exact'</span>
        <span class="comment">%              'plugin' is not available when model.w is an</span>
        <span class="comment">%              mvnInvGammaDist object.</span>
        <span class="comment">%</span>
        <span class="comment">%              'exact' is not available when model.w is a constDist</span>
        <span class="comment">%              object.</span>
        <span class="comment">%</span>
        <span class="comment">%              If not specified, 'plugin' is used if model.w is a</span>
        <span class="comment">%              constDist and 'exact' used otherwise.</span>
        <span class="comment">%</span>
        <span class="comment">% OUTPUT:      The output depends on (1), how the model was fit, and (2)</span>
        <span class="comment">%              on the specified prediction method. When method =</span>
        <span class="comment">%              'plugin', the output is an mvnDist but with constant</span>
        <span class="comment">%              variance sigma2. To obtain a point estimate, simply take</span>
        <span class="comment">%              the mean as in mean(py).</span>
        <span class="comment">%</span>
        <span class="comment">%</span>
        <span class="comment">%</span>
            <span class="keyword">if</span>(nargin == 2 &amp;&amp; isnumeric(varargin{1}))
                X = varargin{1}; method = <span class="string">'default'</span>;
            <span class="keyword">else</span>
                [X,method] = process_options(varargin,<span class="string">'X'</span>,[],<span class="string">'method'</span>,<span class="string">'default'</span>);
            <span class="keyword">end</span>

            <span class="keyword">if</span>(strcmpi(method,<span class="string">'default'</span>))
               <span class="keyword">switch</span> class(model.w)
                   <span class="keyword">case</span> {<span class="string">'mvnDist'</span>,<span class="string">'mvnInvGammaDist'</span>}
                       method = <span class="string">'exact'</span>;
                   <span class="keyword">otherwise</span>
                       method = <span class="string">'plugin'</span>;
               <span class="keyword">end</span>
            <span class="keyword">end</span>

            <span class="keyword">switch</span> method

                <span class="keyword">case</span> <span class="string">'plugin'</span>
                    <span class="keyword">if</span>(isa(obj.w,<span class="string">'mvnInvGammaDist'</span>))
                       error(<span class="string">'Plugin approximation is not supported when the specified prior is an mvnInvGammaDist object. '</span>);
                    <span class="keyword">end</span>
                    <span class="keyword">if</span> ~isempty(model.transformer)
                        X = test(model.transformer, X);
                    <span class="keyword">end</span>
                    n = size(X,1);
                    muHat = X*mode(model.w);
                    sigma2Hat = model.sigma2*ones(n,1); <span class="comment">% constant variance!</span>
                    py = gaussDist(muHat, sigma2Hat);

                <span class="keyword">case</span> <span class="string">'exact'</span>
                    py = predictBayesian(model,X);

                <span class="keyword">otherwise</span>
                    error(<span class="string">'%s is not supported - choose one of ''plugin'' or ''exact'''</span>,method);
            <span class="keyword">end</span>



        <span class="keyword">end</span>

         <span class="keyword">function</span> model = mkRndParams(model, d)
            model.w = randn(d,1);
            model.sigma2 = rand(1,1);
        <span class="keyword">end</span>

        <span class="keyword">function</span> p = logprob(model, X, y)
            <span class="comment">% p(i) = log p(Y(i,:) | X(i,:), params)</span>
            [yhat] = mean(predict(model, X));
            s2 = model.sigma2;
            p = -1/(2*s2)*(y(:)-yhat(:)).^2 - 0.5*log(2*pi*s2);
            <span class="comment">%[yhat, py] = predict(model, X);</span>
            <span class="comment">%PP = logprob(py, y); % PP(i,j) = p(Y(i)| yhat(j))</span>
            <span class="comment">%p1 = diag(PP);</span>
            <span class="comment">%yhat = predict(model, X);</span>
            <span class="comment">%assert(approxeq(p,p1))</span>
        <span class="keyword">end</span>

        <span class="keyword">function</span> p = squaredErr(model, X, y)
            yhat = mean(predict(model, X));
            p  = (y(:)-yhat(:)).^2;
        <span class="keyword">end</span>

        <span class="keyword">function</span> s = bicScore(model, X, y, lambda)
            L = sum(logprob(model, X, y));
            n = size(X,1);
            <span class="comment">%d = length(model.w);</span>
            d = dofRidge(model, X, lambda);
            s = L-0.5*d*log(n);
        <span class="keyword">end</span>

        <span class="keyword">function</span> s = aicScore(model, X, y, lambda)
            L = sum(logprob(model, X, y));
            n = size(X,1);
            <span class="comment">%d = length(model.w);</span>
            d = dofRidge(model, X, lambda);
            s = L-d;
        <span class="keyword">end</span>

        <span class="keyword">function</span> df = dofRidge(model, X, lambdas)
            <span class="comment">% compute the degrees of freedom for a given lambda value</span>
            <span class="comment">% Elements of Statistical Learning p63</span>
            <span class="keyword">if</span> nargin &lt; 3, lambdas = model.lambda; <span class="keyword">end</span>
            <span class="keyword">if</span> ~isempty(model.transformer)
                X = train(model.transformer, X);
                <span class="keyword">if</span> addOffset(model.transformer)
                    X = X(:,2:end);
                <span class="keyword">end</span>
            <span class="keyword">end</span>
            xbar = mean(X);
            XC = X - repmat(xbar,size(X,1),1);
            [U,D,V] = svd(XC,<span class="string">'econ'</span>);
            D2 = diag(D.^2);
            <span class="keyword">for</span> i=1:length(lambdas)
                df(i) = sum(D2./(D2+lambdas(i)));
            <span class="keyword">end</span>
        <span class="keyword">end</span>

    <span class="keyword">end</span>
</pre><pre class="codeoutput">ans = 
  LinregDist

  Properties:
              w: []
         sigma2: []
    transformer: []
         ndimsX: []
         ndimsY: 1</pre><h2>Protected Methods<a name="4"></a></h2><pre class="codeinput">    methods(Access = <span class="string">'protected'</span>)

        <span class="keyword">function</span> model = fitMAP(model, varargin)
            <span class="comment">% m = fit(model, 'name1', val1, 'name2', val2, ...)</span>
            <span class="comment">% Arguments are</span>
            <span class="comment">% 'X' - X(i,:) Do NOT include a column of 1's</span>
            <span class="comment">% 'y'- y(i)</span>
            <span class="comment">% 'prior' - one of {'none', 'L2', 'L1'}</span>
            <span class="comment">% 'lambda' &gt;= 0</span>
            <span class="comment">% algorithm - must be one of { ridgeQR, ridgeSVD }.</span>
            [X, y, method, lambda, prior] = process_options(<span class="keyword">...</span>
                varargin, <span class="string">'X'</span>, [], <span class="string">'y'</span>, [], <span class="string">'algorithm'</span>, <span class="string">'ridgeQR'</span>, <span class="keyword">...</span>
                <span class="string">'lambda'</span>, 0, <span class="string">'prior'</span>, <span class="string">'none'</span>);
            <span class="keyword">if</span> lambda&gt;0 &amp;&amp; strcmpi(prior, <span class="string">'none'</span>), prior = <span class="string">'L2'</span>; <span class="keyword">end</span>
            <span class="keyword">if</span> ~isempty(model.transformer)
                [X, model.transformer] = train(model.transformer, X);
            <span class="keyword">end</span>
            <span class="keyword">switch</span> lower(prior)
                <span class="keyword">case</span> <span class="string">'none'</span>
                    n = size(X,1);
                    model.w = X \ y;
                    yhat = X*model.w;
                    model.sigma2 = mean((yhat-y).^2); <span class="comment">% 1/n, not unbiased</span>

                <span class="keyword">case</span> <span class="string">'l2'</span>
                    <span class="keyword">if</span> ~isempty(model.transformer) &amp;&amp; addOffset(model.transformer)
                        X = X(:,2:end); <span class="comment">% remove leading column of 1s</span>
                        addOnes = true;
                    <span class="keyword">else</span>
                        addOnes = false;
                    <span class="keyword">end</span>
                    model.w = ridgereg(X, y, lambda, algorithm, addOnes);
                    n = size(X,1);
                    <span class="keyword">if</span> addOnes
                        X = [ones(n,1) X]; <span class="comment">% column of 1s for w0 term</span>
                    <span class="keyword">end</span>
                    yhat = X*model.w;
                    model.sigma2 = mean((yhat-y).^2); <span class="comment">% 1/n, not unbiased</span>

                <span class="keyword">case</span> <span class="string">'l1'</span>
                    <span class="comment">% lasso</span>
                    error(<span class="string">'not yet implemented'</span>)

                <span class="keyword">case</span> <span class="string">'l1l2'</span>
                    <span class="comment">% elastic net</span>
                    error(<span class="string">'not yet implemented'</span>);
                <span class="keyword">otherwise</span>
                    error([<span class="string">'unrecognized method '</span> method])
            <span class="keyword">end</span>
        <span class="keyword">end</span>


        <span class="keyword">function</span> model = fitBayesian(model, varargin)
            <span class="comment">% m = inferParams(model, 'name1', val1, 'name2', val2, ...)</span>
            <span class="comment">% Arguments are</span>
            <span class="comment">% 'X' - X(i,:) Do NOT include a column of 1's</span>
            <span class="comment">% 'y'- y(i)</span>
            <span class="comment">% lambda &gt;= 0</span>
            <span class="comment">% 'prior' - one of {mvnDist object, mvnInvGammaDist object, ...</span>
            <span class="comment">%                   'mvn', 'mvnIG'}</span>
            <span class="comment">% In the latter 2 cases, we create a diagonal Gaussian prior</span>
            <span class="comment">% with precision lambda (except for the offset term)</span>
            [X, y, lambda, prior, sigma2] = process_options(<span class="keyword">...</span>
                varargin, <span class="string">'X'</span>, [], <span class="string">'y'</span>, [], <span class="string">'lambda'</span>, 1e-3, <span class="string">'prior'</span>, <span class="string">'mvn'</span>,<span class="keyword">...</span>
                <span class="string">'sigma2'</span>, []);
            <span class="keyword">if</span> ~isempty(model.transformer)
                [X, model.transformer] = train(model.transformer, X);
            <span class="keyword">end</span>
            <span class="keyword">if</span> isa(prior, <span class="string">'char'</span>)
                model.w = makeSphericalPrior(model, X, lambda, prior);
            <span class="keyword">end</span>
            <span class="keyword">if</span> ~isempty(sigma2)
                <span class="comment">% this is ignored if the prior is mvnIG</span>
                model.sigma2 = sigma2;
            <span class="keyword">end</span>
            done = false;
            <span class="keyword">switch</span> class(model.w)
                <span class="keyword">case</span> <span class="string">'mvnDist'</span>
                    <span class="keyword">if</span> isa(model.sigma2, <span class="string">'double'</span>) &amp;&amp; model.sigma2 &gt; 0
                        <span class="comment">% conjugate updating of w with fixed sigma2</span>
                        S0 = model.w.Sigma; w0 = model.w.mu;
                        s2 = model.sigma2; sigma = sqrt(s2);
                        Lam0 = inv(S0); <span class="comment">% yuck!</span>
                        [wn, Sn] = normalEqnsBayes(X, y, Lam0, w0, sigma);
                        model.w = mvnDist(wn, Sn);
                        done = true;
                    <span class="keyword">end</span>
                <span class="keyword">case</span> <span class="string">'mvnInvGammaDist'</span>
                    <span class="comment">% conjugate updating with unknown mu and sigma2</span>
                    model.w = updateMVNIG(model, X, y);
                    done = true;
            <span class="keyword">end</span>
            assert(done)
        <span class="keyword">end</span>

        <span class="keyword">function</span> [py] = predictBayesian(model, X)
            <span class="keyword">if</span> ~isempty(model.transformer)
                X = test(model.transformer, X);
            <span class="keyword">end</span>
            n = size(X,1);
            done = false;
            <span class="keyword">switch</span> class(model.w)
                <span class="keyword">case</span> <span class="string">'mvnDist'</span>
                    <span class="keyword">if</span> isa(model.sigma2, <span class="string">'double'</span>)
                        muHat = X*model.w.mu;
                        Sn = model.w.Sigma;
                        sigma2Hat = model.sigma2*ones(n,1) + diag(X*Sn*X');
<span class="comment">                        %{
</span><span class="comment">              for i=1:n
</span><span class="comment">                xi = X(i,:)';
</span><span class="comment">                s2(i) = model.sigma2 + xi'*Sn*xi;
</span><span class="comment">              end
</span><span class="comment">              assert(approxeq(sigma2Hat, s2))
</span><span class="comment">                        %}
</span>                        py = gaussDist(muHat, sigma2Hat);
                        done = true;
                    <span class="keyword">end</span>
                <span class="keyword">case</span> <span class="string">'mvnInvGammaDist'</span>
                    wn = model.w.mu;
                    Sn = model.w.Sigma;
                    vn = model.w.a*2;
                    sn2 = 2*model.w.b/vn;
                    m = size(X,n);
                    SS = sn2*(eye(m) + X*Sn*X');
                    py = studentDist(vn, X*wn, diag(SS));
                    done = true;
            <span class="keyword">end</span>
            assert(done)
        <span class="keyword">end</span>

    <span class="keyword">end</span>
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><p class="footer"><br>
            Published with MATLAB&reg; 7.7<br></p>
      </div>
      <!--
##### SOURCE BEGIN #####
classdef LinregDist < condProbDist
%% Linear Regression Conditional Distribution


    properties
        w;                   % w is the posterior distribution of the weights. 
                             % The form depends on how this object was fit. 
                             % If method = 'map', the default, then w 
                             % represents the MAP estimate and is stored as a 
                             % constDist object. If method = 'bayesian', then
                             % the form is conjugate to the prior, i.e. either
                             % an mvnDist object in the case that the variance,
                             % sigma2, is known, or an mvnInvGammaDist object in
                             % the case that both the mean and variance are
                             % unknown. 
                             
        sigma2;              % Fixed/Known value for the variance in 
                             % p(y | w,X) = N(y | Xw, sigma2*I). This value is
                             % ignored when an mvnInvGammaDist prior is
                             % specified. 
                             
        transformer;         % A data transformer object, e.g. kernelTransformer
    end

    %% Main methods
    methods
        function model = LinregDist(varargin)
        %Constructor
        %
        % FORMAT:
        %           model = LinRegDist('name1',val1,'name2',val2,...);
        %       
        %
        % INPUT: 
        % 
        % 'transformer'     - An optional transformer object such as  kernelTransformer
        %
        % 'w'               - The posterior of the weights: either a constDist,
        %                     mvnDist or mvnInvGammaDist object. This will
        %                     usually be learned when fitting the object and can
        %                     be left unspecified. If w is a numeric matrix, it
        %                     is converted to a constDist object. 
        %
        % 'sigma2'          - Fixed/Known value for the variance in 
        %                     p(y | w,X) = N(y | Xw, sigma2*I). This is only used
        %                     when the fit method is 'bayesian' and the specified 
        %                     prior is an mvnDist object. When the object
        %                     is fit using method = MAP, this is estimated as
        %                     mean( (yhat-y).^2). This can also be specified
        %                     when fitting the model. 
        %
        % OUTPUT:           
        %
        % model             - The constructed LinregDist object
        %
        
            [transformer, w, sigma2] = process_options(varargin,...
                        'transformer', []                      ,...      
                        'w'          , []                      ,... 
                        'sigma2'     , []                      );
                    
            m.transformer = transformer;
            if(isnumeric(w) && ~isempty(w))
                model.w = constDist(w);
            else
                model.w = w;
            end
            model.sigma2 = sigma2;
        end

       
        function model = fit(model,varargin)
        % Compute the posterior distribution over w, the weights. This is either
        % a constant distribution representing the MAP estimate if method =
        % 'map', (the default), or conjugate to the specified prior object, i.e.
        % one of mvnDist or mvnInvGammaDist. 
        %
        % FORMAT: 
        %           model = fit(model,'name1',val1,'name2',val2,...)
        %
        % INPUT
        %
        % 'X'      - The training examples: X(i,:) is the ith case
        %
        % 'y'      - The output or target values s.t. y(i,:) corresponds to
        %            X(i,:); y may be multidimensional. 
        %
        % 'prior'  - In the case that method = 'map' estimation,(default), one
        %            of four strings can be specified, 'none' | 'L1' | 'L2' | 'L1L2' 
        %            where 'none' corresponds to the mle and 'L1L2' corresponds
        %            to elastic net. 
        %
        %            In the case that method = 'bayesian', you can specify a
        %            preconstructed mvnDist object or an mvnInvGammaDist object.
        %            Alternatively, you can specify the string 'mvn' or 'mvnIG'
        %            and an object will be constructed for you. In these latter 
        %            two cases, the prior is spherical with lambda used as 
        %            the precision, (except for the offset terms if any) and
        %            other values set to reasonable defaults.
        %
        % 'lambda'   In the case of map estimation, this is the regularization
        %            value. In the case of 'bayesian' estimation where the
        %            'prior' was specified as a string, this acts as the
        %            precision. 
        % 
        % 'method'   One of ['map' | 'bayesian']
        %
        % 'algorithm' Only used when method is 'map'
        %
        %            case: {'L1','L1L2'}  ['lars'] | 'shooting'
        %            case: {'L2'}         ['ridgeQR'] | 'ridgeSVD']
        %               
        %                  
            method = process_options(varargin, 'method' , 'map');     
            switch lower(method)
                
                case {'map','mle'}
                   model = fitMAP(model,varargin{:});
                case 'bayesian'
                   model = fitBayesian(model,varargin{:});
                otherwise
                    error('%s is not supported - choose one of ''map'' or ''bayesian''',method);
            end
            
            
        end

        function py = predict(model, varargin)
        % Return a predictive distribution given the fitted model and the
        % test set examples, X. 
        %
        % FORMAT:
        %
        % model = predict(model,'name1',val1,'name2',val2,...);
        %
        % OR
        %
        % model = predict(model,X)
        %
        % INPUT:
        % 
        % 'X'        - The test set examples, X(i,:) is example i
        % 
        % 'method'   - ['plugin'] | 'exact'  
        %              'plugin' is not available when model.w is an
        %              mvnInvGammaDist object. 
        %              
        %              'exact' is not available when model.w is a constDist
        %              object.
        %
        %              If not specified, 'plugin' is used if model.w is a
        %              constDist and 'exact' used otherwise. 
        % 
        % OUTPUT:      The output depends on (1), how the model was fit, and (2)
        %              on the specified prediction method. When method =
        %              'plugin', the output is an mvnDist but with constant
        %              variance sigma2. To obtain a point estimate, simply take
        %              the mean as in mean(py). 
        %
        % 
        % 
            if(nargin == 2 && isnumeric(varargin{1}))
                X = varargin{1}; method = 'default';
            else
                [X,method] = process_options(varargin,'X',[],'method','default');
            end
            
            if(strcmpi(method,'default'))
               switch class(model.w)
                   case {'mvnDist','mvnInvGammaDist'}
                       method = 'exact';
                   otherwise 
                       method = 'plugin';
               end
            end

            switch method

                case 'plugin'
                    if(isa(obj.w,'mvnInvGammaDist'))
                       error('Plugin approximation is not supported when the specified prior is an mvnInvGammaDist object. '); 
                    end
                    if ~isempty(model.transformer)
                        X = test(model.transformer, X);
                    end
                    n = size(X,1);
                    muHat = X*mode(model.w);
                    sigma2Hat = model.sigma2*ones(n,1); % constant variance!
                    py = gaussDist(muHat, sigma2Hat);

                case 'exact'
                    py = predictBayesian(model,X);
                    
                otherwise
                    error('%s is not supported - choose one of ''plugin'' or ''exact''',method);
            end



        end
  
         function model = mkRndParams(model, d)
            model.w = randn(d,1);
            model.sigma2 = rand(1,1);
        end

        function p = logprob(model, X, y)
            % p(i) = log p(Y(i,:) | X(i,:), params)
            [yhat] = mean(predict(model, X));
            s2 = model.sigma2;
            p = -1/(2*s2)*(y(:)-yhat(:)).^2 - 0.5*log(2*pi*s2);
            %[yhat, py] = predict(model, X);
            %PP = logprob(py, y); % PP(i,j) = p(Y(i)| yhat(j))
            %p1 = diag(PP);
            %yhat = predict(model, X);
            %assert(approxeq(p,p1))
        end

        function p = squaredErr(model, X, y)
            yhat = mean(predict(model, X));
            p  = (y(:)-yhat(:)).^2;
        end

        function s = bicScore(model, X, y, lambda)
            L = sum(logprob(model, X, y));
            n = size(X,1);
            %d = length(model.w);
            d = dofRidge(model, X, lambda);
            s = L-0.5*d*log(n);
        end

        function s = aicScore(model, X, y, lambda)
            L = sum(logprob(model, X, y));
            n = size(X,1);
            %d = length(model.w);
            d = dofRidge(model, X, lambda);
            s = L-d;
        end

        function df = dofRidge(model, X, lambdas)
            % compute the degrees of freedom for a given lambda value
            % Elements of Statistical Learning p63
            if nargin < 3, lambdas = model.lambda; end
            if ~isempty(model.transformer)
                X = train(model.transformer, X);
                if addOffset(model.transformer)
                    X = X(:,2:end);
                end
            end
            xbar = mean(X);
            XC = X - repmat(xbar,size(X,1),1);
            [U,D,V] = svd(XC,'econ');
            D2 = diag(D.^2);
            for i=1:length(lambdas)
                df(i) = sum(D2./(D2+lambdas(i)));
            end
        end

    end

    %% Protected Methods
    methods(Access = 'protected')

        function model = fitMAP(model, varargin)
            % m = fit(model, 'name1', val1, 'name2', val2, ...)
            % Arguments are
            % 'X' - X(i,:) Do NOT include a column of 1's
            % 'y'- y(i)
            % 'prior' - one of {'none', 'L2', 'L1'}
            % 'lambda' >= 0
            % algorithm - must be one of { ridgeQR, ridgeSVD }.
            [X, y, method, lambda, prior] = process_options(...
                varargin, 'X', [], 'y', [], 'algorithm', 'ridgeQR', ...
                'lambda', 0, 'prior', 'none');
            if lambda>0 && strcmpi(prior, 'none'), prior = 'L2'; end
            if ~isempty(model.transformer)
                [X, model.transformer] = train(model.transformer, X);
            end
            switch lower(prior)
                case 'none'
                    n = size(X,1);
                    model.w = X \ y;
                    yhat = X*model.w;
                    model.sigma2 = mean((yhat-y).^2); % 1/n, not unbiased

                case 'l2'
                    if ~isempty(model.transformer) && addOffset(model.transformer)
                        X = X(:,2:end); % remove leading column of 1s
                        addOnes = true;
                    else
                        addOnes = false;
                    end
                    model.w = ridgereg(X, y, lambda, algorithm, addOnes);
                    n = size(X,1);
                    if addOnes
                        X = [ones(n,1) X]; % column of 1s for w0 term
                    end
                    yhat = X*model.w;
                    model.sigma2 = mean((yhat-y).^2); % 1/n, not unbiased

                case 'l1'
                    % lasso
                    error('not yet implemented')
                    
                case 'l1l2'
                    % elastic net
                    error('not yet implemented');
                otherwise
                    error(['unrecognized method ' method])
            end
        end


        function model = fitBayesian(model, varargin)
            % m = inferParams(model, 'name1', val1, 'name2', val2, ...)
            % Arguments are
            % 'X' - X(i,:) Do NOT include a column of 1's
            % 'y'- y(i)
            % lambda >= 0
            % 'prior' - one of {mvnDist object, mvnInvGammaDist object, ...
            %                   'mvn', 'mvnIG'}
            % In the latter 2 cases, we create a diagonal Gaussian prior
            % with precision lambda (except for the offset term)
            [X, y, lambda, prior, sigma2] = process_options(...
                varargin, 'X', [], 'y', [], 'lambda', 1e-3, 'prior', 'mvn',...
                'sigma2', []);
            if ~isempty(model.transformer)
                [X, model.transformer] = train(model.transformer, X);
            end
            if isa(prior, 'char')
                model.w = makeSphericalPrior(model, X, lambda, prior);
            end
            if ~isempty(sigma2)
                % this is ignored if the prior is mvnIG
                model.sigma2 = sigma2;
            end
            done = false;
            switch class(model.w)
                case 'mvnDist'
                    if isa(model.sigma2, 'double') && model.sigma2 > 0
                        % conjugate updating of w with fixed sigma2
                        S0 = model.w.Sigma; w0 = model.w.mu;
                        s2 = model.sigma2; sigma = sqrt(s2);
                        Lam0 = inv(S0); % yuck!
                        [wn, Sn] = normalEqnsBayes(X, y, Lam0, w0, sigma);
                        model.w = mvnDist(wn, Sn);
                        done = true;
                    end
                case 'mvnInvGammaDist'
                    % conjugate updating with unknown mu and sigma2
                    model.w = updateMVNIG(model, X, y);
                    done = true;
            end
            assert(done)
        end

        function [py] = predictBayesian(model, X)
            if ~isempty(model.transformer)
                X = test(model.transformer, X);
            end
            n = size(X,1);
            done = false;
            switch class(model.w)
                case 'mvnDist'
                    if isa(model.sigma2, 'double')
                        muHat = X*model.w.mu;
                        Sn = model.w.Sigma;
                        sigma2Hat = model.sigma2*ones(n,1) + diag(X*Sn*X');
                        %{
              for i=1:n
                xi = X(i,:)';
                s2(i) = model.sigma2 + xi'*Sn*xi;
              end
              assert(approxeq(sigma2Hat, s2))
                        %}
                        py = gaussDist(muHat, sigma2Hat);
                        done = true;
                    end
                case 'mvnInvGammaDist'
                    wn = model.w.mu;
                    Sn = model.w.Sigma;
                    vn = model.w.a*2;
                    sn2 = 2*model.w.b/vn;
                    m = size(X,n);
                    SS = sn2*(eye(m) + X*Sn*X');
                    py = studentDist(vn, X*wn, diag(SS));
                    done = true;
            end
            assert(done)
        end

    end

end
##### SOURCE END #####
-->
   </body>
</html>