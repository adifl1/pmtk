
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN">
<html xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      -->
      <title>Logistic Regression: Visualizing the Predictive Distribution</title>
      <meta name="generator" content="MATLAB 7.6">
      <meta name="date" content="2008-10-21">
      <meta name="m-file" content="LRvisualizePredictive"><style>

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head>
   <body>
      <div class="content">
         <h1>Logistic Regression: Visualizing the Predictive Distribution</h1>
         <introduction>
            <p>Here we fit a logistic regression model to synthetic data and visualize the predictive distribution. We use an L2 regularizer
               and perform an RBF expansion of the data.
            </p>
         </introduction>
         <h2>Contents</h2>
         <div>
            <ul>
               <li><a href="#1">Load and Plot the Data</a></li>
               <li><a href="#2">Create the Data Transformer</a></li>
               <li><a href="#3">Create the Model</a></li>
               <li><a href="#4">Fit the Model</a></li>
               <li><a href="#7">Predict</a></li>
               <li><a href="#9">Plot the Predictive Distribution</a></li>
            </ul>
         </div>
         <h2>Load and Plot the Data<a name="1"></a></h2>
         <p>Our synthetic data consists of 300 2D examples from two different classes, 1 and 2.</p><pre class="codeinput">load <span class="string">synthetic2Ddata</span>
figure;
plot(X(Y==1,1),X(Y==1,2),<span class="string">'.r'</span>,<span class="string">'MarkerSize'</span>,20); hold <span class="string">on</span>;
plot(X(Y==2,1),X(Y==2,2),<span class="string">'.b'</span>,<span class="string">'MarkerSize'</span>,20);
set(gca,<span class="string">'XTick'</span>,0:0.5:1,<span class="string">'YTick'</span>,0:0.5:1);
title(<span class="string">'Training Data'</span>);
legend({<span class="string">'Class1'</span>,<span class="string">'Class2'</span>},<span class="string">'Location'</span>,<span class="string">'BestOutside'</span>);
</pre><img vspace="5" hspace="5" src="LRvisualizePredictive_01.png"> <h2>Create the Data Transformer<a name="2"></a></h2>
         <p>We will make use of PMTK's transformer objects to easily preprocess the data and perform the basis expansion. We chain three
            transformers together, which will be applied to the data in sequence. When we pass our chainTransformer to our model, (which
            we will create shortly), all of the details of the transformation are retained, and where appropriate, applied to future test
            data.
         </p><pre class="codeinput">sigma2 = 1;          <span class="comment">% kernel bandwidth</span>
T = chainTransformer({standardizeTransformer(false)      ,<span class="keyword">...</span>
                      kernelTransformer(<span class="string">'rbf'</span>,sigma2)} );
</pre><h2>Create the Model<a name="3"></a></h2>
         <p>We now create a new logistic regression model and pass it the transformer object we just created.</p><pre class="codeinput">model = multinomLogregDist(<span class="string">'nclasses'</span>,2, <span class="string">'transformer'</span>, T);
</pre><h2>Fit the Model<a name="4"></a></h2>
         <p>To fit the model, we simply call the model's fit method and pass in the data. Here we use an L2 regularizer, however, an L1
            sparsity promoting regularizer could have been used just as easily by replacing the string 'l2' with 'l1'.
         </p><pre class="codeinput">lambda = 1e-3;                                              <span class="comment">% L2 regularizer</span>
model = fit(model,<span class="string">'prior'</span>,<span class="string">'l2'</span>,<span class="string">'lambda'</span>,lambda,<span class="string">'X'</span>,X,<span class="string">'y'</span>,Y);
</pre><p>We can specify which optimization method we would like to use by passing in its name to the fit method as in the following.
            There are number of options but reasonable defaults exist.
         </p><pre>model = fit(model,'prior','l2','lambda',lambda,'X',X,'y',Y,'method','lbfgs');</pre><h2>Predict<a name="7"></a></h2>
         <p>To visualize the predictive distribution we will first create grid of points in our original 2D feature space and evaluate
            the posterior probability that each point belongs to class 1.
         </p><pre class="codeinput">[X1grid, X2grid] = meshgrid(0:0.01:1,0:0.01:1);
[nrows,ncols] = size(X1grid);
testData = [X1grid(:),X2grid(:)];
</pre><p>The output of the predict method is a discrete distribution over the class labels. We extract the probabilities of each test
            point belonging to class 1 and reshape the vector for plotting purposes.
         </p><pre class="codeinput">pred = predict(model,testData);              <span class="comment">% pred is an object - a discrete distribution</span>
pclass1 = pred.probs(:,1);
probGrid = reshape(pclass1,nrows,ncols);
</pre><h2>Plot the Predictive Distribution<a name="9"></a></h2>
         <p>We can now make use of Matlab's excellent plotting capabilities and plot the surface of the distribution.</p><pre class="codeinput">figure;
surf(X1grid,X2grid,probGrid);
shading <span class="string">interp</span>; view([0 90]); colorbar;
set(gca,<span class="string">'XTick'</span>,0:0.5:1,<span class="string">'YTick'</span>,0:0.5:1);
title(<span class="string">'Predictive Distribution'</span>);
</pre><img vspace="5" hspace="5" src="LRvisualizePredictive_02.png"> <p class="footer"><br>
            Published with MATLAB&reg; 7.6<br></p>
      </div>
      <!--
##### SOURCE BEGIN #####
%% Logistic Regression: Visualizing the Predictive Distribution
% Here we fit a logistic regression model to synthetic data and visualize the
% predictive distribution. We use an L2 regularizer and perform an RBF expansion
% of the data. 

%% Load and Plot the Data
% Our synthetic data consists of 300 2D examples from two different classes, 1 and 2. 
load synthetic2Ddata
figure;
plot(X(Y==1,1),X(Y==1,2),'.r','MarkerSize',20); hold on;
plot(X(Y==2,1),X(Y==2,2),'.b','MarkerSize',20);
set(gca,'XTick',0:0.5:1,'YTick',0:0.5:1);
title('Training Data');
legend({'Class1','Class2'},'Location','BestOutside');
%% Create the Data Transformer
% We will make use of PMTK's transformer objects to easily preprocess the data
% and perform the basis expansion. We chain three transformers together, which
% will be applied to the data in sequence. When we pass our chainTransformer to
% our model, (which we will create shortly), all of the details of the
% transformation are retained, and where appropriate, applied to future test data.
%
sigma2 = 1;          % kernel bandwidth
T = chainTransformer({standardizeTransformer(false)      ,...
                      kernelTransformer('rbf',sigma2)} );
%% Create the Model
% We now create a new logistic regression model and pass it the transformer object
% we just created.
model = multinomLogregDist('nclasses',2, 'transformer', T);
%% Fit the Model
% To fit the model, we simply call the model's fit method and pass in the data.
% Here we use an L2 regularizer, however, an L1 sparsity promoting regularizer
% could have been used just as easily by replacing the string 'l2' with 'l1'.
lambda = 1e-3;                                              % L2 regularizer
model = fit(model,'prior','l2','lambda',lambda,'X',X,'y',Y);
%%
% We can specify which optimization method we would like to use by passing in
% its name to the fit method as in the following. There are number of options
% but reasonable defaults exist. 
%%
%  model = fit(model,'prior','l2','lambda',lambda,'X',X,'y',Y,'method','lbfgs');
%% Predict
% To visualize the predictive distribution we will first create grid of points
% in our original 2D feature space and evaluate the posterior probability that
% each point belongs to class 1. 
%
[X1grid, X2grid] = meshgrid(0:0.01:1,0:0.01:1);
[nrows,ncols] = size(X1grid);
testData = [X1grid(:),X2grid(:)];
%%
% The output of the predict method is a discrete distribution over the class
% labels. We extract the probabilities of each test point belonging to class 1
% and reshape the vector for plotting purposes. 
pred = predict(model,testData);              % pred is an object - a discrete distribution
pclass1 = pred.probs(:,1);                   
probGrid = reshape(pclass1,nrows,ncols);
%% Plot the Predictive Distribution
% We can now make use of Matlab's excellent plotting capabilities and plot the
% surface of the distribution. 
figure;
surf(X1grid,X2grid,probGrid);
shading interp; view([0 90]); colorbar;
set(gca,'XTick',0:0.5:1,'YTick',0:0.5:1);
title('Predictive Distribution');
%%
##### SOURCE END #####
-->
   </body>
</html>