
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN">
<html xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      -->
      <title>multinomLogregDist</title>
      <meta name="generator" content="MATLAB 7.6">
      <meta name="date" content="2008-10-19">
      <meta name="m-file" content="multinomLogregDist"><style>

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head>
   <body>
      <div class="content">
         <h2>Contents</h2>
         <div>
            <ul>
               <li><a href="#3">Main methods</a></li>
               <li><a href="#4">Demos</a></li>
            </ul>
         </div><pre class="codeinput"><span class="keyword">classdef</span> multinomLogregDist &lt; condProbDist
</pre><pre class="codeinput">  properties
    w;
    transformer;
    nclasses;
  <span class="keyword">end</span>
</pre><h2>Main methods<a name="3"></a></h2><pre class="codeinput">  methods
    <span class="keyword">function</span> m = multinomLogregDist(varargin)
      [transformer,  w, C] = process_options(<span class="keyword">...</span>
        varargin, <span class="string">'transformer'</span>, [], <span class="string">'w'</span>, [], <span class="string">'nclasses'</span>, []);
      m.transformer = transformer;
      m.w = w;
      m.nclasses = C;
    <span class="keyword">end</span>


     <span class="keyword">function</span> [obj, output] = fit(obj, varargin)
       <span class="comment">% model = fit(model, 'name1', val1, 'name2', val2, ...)</span>
       <span class="comment">% Arguments are</span>
       <span class="comment">% 'X' - X(i,:) Do NOT include a column of 1's</span>
       <span class="comment">% 'y'- y(i) in {1,...,K}</span>
       <span class="comment">% 'prior' - one of { 'none', 'L2', 'L1'}</span>
       <span class="comment">% 'lambda' - &gt;= 0</span>
       <span class="comment">% 'method' - one of {'boundoptRelaxed', 'boundoptStepwise', any minFunc method}</span>
       output = [];
       [X, y,  prior, lambda, method] = process_options(<span class="keyword">...</span>
         varargin, <span class="string">'X'</span>, [], <span class="string">'y'</span>, [],  <span class="string">'prior'</span>, <span class="string">'none'</span>, <span class="string">'lambda'</span>, 0, <span class="keyword">...</span>
         <span class="string">'method'</span>, <span class="string">'boundopt'</span>);
       <span class="keyword">if</span> lambda &gt; 0 &amp;&amp; strcmpi(prior, <span class="string">'none'</span>), prior = <span class="string">'L2'</span>; <span class="keyword">end</span>
       <span class="keyword">if</span> ~isempty(obj.transformer)
         [X, obj.transformer] = train(obj.transformer, X);
       <span class="keyword">end</span>
       [n d] = size(X);
       C = obj.nclasses;
       <span class="keyword">if</span> isempty(C), C = length(unique(y)); <span class="keyword">end</span>
       Y1 = oneOfK(y, C);
       <span class="keyword">switch</span> lower(prior)
         <span class="keyword">case</span> {<span class="string">'l2'</span>, <span class="string">'none'</span>}
           options = optimset(<span class="string">'Display'</span>,<span class="string">'none'</span>,<span class="string">'Diagnostics'</span>,<span class="string">'off'</span>,<span class="string">'GradObj'</span>,<span class="string">'on'</span>,<span class="keyword">...</span>
             <span class="string">'Hessian'</span>,<span class="string">'on'</span>,<span class="string">'DerivativeCheck'</span>,<span class="string">'off'</span>);
           <span class="keyword">switch</span> lower(method)
             <span class="keyword">case</span> <span class="string">'boundoptrelaxed'</span>
               <span class="comment">%obj.w = compileAndRun('boundOptL2overrelaxed',X,Y1,lambda);</span>
               [obj.w, output] = boundOptL2overrelaxed(X, Y1, lambda);
             <span class="keyword">case</span> <span class="string">'boundoptstepwise'</span>
                 [obj.w, output] = boundOptL2Stepwise(X, Y1, lambda);
             <span class="keyword">otherwise</span>
               options.Method = method;
               winit = zeros(d*(C-1),1);
               [obj.w, f, exitflag, output] = minFunc(@multinomLogregNLLgradHessL2, winit, options, X, Y1, lambda);
           <span class="keyword">end</span>
         <span class="keyword">case</span> {<span class="string">'l1'</span>}
           <span class="keyword">switch</span> method
             <span class="keyword">case</span> <span class="string">'boundopt'</span>
               obj.w = boundOptL1overrelaxed(X, Y1, lambda);
             <span class="keyword">otherwise</span>
               error([<span class="string">'unrecognized method '</span> method])
           <span class="keyword">end</span>
         <span class="keyword">otherwise</span>
           error([<span class="string">'unrecognized prior '</span> prior])
       <span class="keyword">end</span>
     <span class="keyword">end</span>

      <span class="keyword">function</span> p = logprob(obj, X, y)
       <span class="comment">% p(i) = log p(Y(i) | X(i,:), params), Y(i) in 1...C</span>
       pred = predict(obj, X);
       P = pred.probs;
       Y = oneOfK(y, obj.nclasses);
       p =  sum(sum(Y.*log(P)));
     <span class="keyword">end</span>

     <span class="keyword">function</span> pred = predict(obj, X, w)
       <span class="comment">% X(i,:) is case i</span>
       <span class="comment">% pred(i) = DiscreteDist(y|X(i,:))</span>
       <span class="keyword">if</span> nargin &lt; 3, w = obj.w; <span class="keyword">end</span>
       <span class="keyword">if</span> ~isempty(obj.transformer)
         X = test(obj.transformer, X);
       <span class="keyword">end</span>
       P = multiSigmoid(X,w);
       pred = discreteDist(P);
     <span class="keyword">end</span>

  <span class="keyword">end</span>
</pre><pre class="codeoutput">ans = 
multinomLogregDist
properties:
              w: []
    transformer: []
       nclasses: []
         ndimsX: []
         ndimsY: 1</pre><h2>Demos<a name="4"></a></h2><pre class="codeinput">  methods(Static = true)

    <span class="keyword">function</span> test()
      <span class="comment">% check functions are syntactically correct</span>
      n = 10; d = 3; C = 4;
      X = randn(n,d );
      y = sampleDiscrete((1/C)*ones(1,C), n, 1);
      m = multinomLogregDist(<span class="string">'nclasses'</span>, C);
      m = fit(m, <span class="string">'X'</span>, X, <span class="string">'y'</span>, y);
      pred = predict(obj, X);
      ll = logprob(obj, X, y);
    <span class="keyword">end</span>

    <span class="keyword">function</span> demoCrabs()
      [Xtrain, ytrain, Xtest, ytest] = makeCrabs;
      sigma2 = 32/5;
      T = chainTransformer({standardizeTransformer(false), kernelTransformer(<span class="string">'rbf'</span>, sigma2)});
      m = multinomLogregDist(<span class="string">'nclasses'</span>, 2, <span class="string">'transformer'</span>, T);
      lambda = 1e-3;
      m = fit(m, <span class="string">'X'</span>, Xtrain, <span class="string">'y'</span>, ytrain, <span class="string">'lambda'</span>, lambda, <span class="string">'method'</span>, <span class="string">'boundopt'</span>);
      <span class="comment">%m = fit(m, 'X', Xtrain, 'y', ytrain, 'lambda', lambda, 'method', 'newton');</span>
      m.w(:)'
      P = predict(m, Xtest);
      yhat = mode(P); <span class="comment">% 1 or 2</span>
      errs = find(yhat(:) ~= ytest(:));
      nerrs = length(errs)
      <span class="comment">%figure;plot(P.probs(:,1))</span>
    <span class="keyword">end</span>

    <span class="keyword">function</span> demoOptimizer()
      setSeed(1);
      load <span class="string">soy</span>; <span class="comment">% n=307, d = 35, C = 3;</span>
      <span class="comment">%load car; % n=1728, d = 6, C = 3;</span>
      methods = {<span class="string">'bb'</span>,  <span class="string">'cg'</span>, <span class="string">'lbfgs'</span>, <span class="string">'newton'</span>,  <span class="string">'boundoptRelaxed'</span>};
      lambda = 1e-3;
      figure; hold <span class="string">on</span>;
      [styles, colors, symbols] =  plotColors;
      <span class="keyword">for</span> mi=1:length(methods)
        tic
        [m, output{mi}] = fit(multinomLogregDist, <span class="string">'X'</span>, X, <span class="string">'y'</span>, Y, <span class="keyword">...</span>
          <span class="string">'lambda'</span>, lambda, <span class="string">'method'</span>, methods{mi});
        T = toc
        time(mi) = T;
        w{mi} = m.w;
        niter = length(output{mi}.ftrace)
        h(mi) = plot(linspace(0, T, niter), output{mi}.ftrace, styles{mi});
        legendstr{mi}  = sprintf(<span class="string">'%s'</span>, methods{mi});
      <span class="keyword">end</span>
      legend(legendstr)
    <span class="keyword">end</span>

<span class="comment">    %{
</span><span class="comment">    function demoMnist()
</span><span class="comment">      load('mnistALL')
</span><span class="comment">      % train_images: [28x28x60000 uint8]
</span><span class="comment">      % test_images: [28x28x10000 uint8]
</span><span class="comment">      % train_labels: [60000x1 uint8]
</span><span class="comment">      % test_labels: [10000x1 uint8]
</span><span class="comment">      setSeed(0);
</span><span class="comment">      Ntrain = 100;
</span><span class="comment">      Ntest = 1000;
</span><span class="comment">      Xtrain = zeros(10, Ntrain, 784);
</span><span class="comment">      ytrain = zeros(10, Ntrain);
</span><span class="comment">      Xtest = zeros(10, Ntrain, 784);
</span><span class="comment">      ytest = zeros(10, Ntest);
</span><span class="comment">      for c=1:10
</span><span class="comment">        ndx = find(mnist.train_labels==c);
</span><span class="comment">        ndx = ndx(1:Ntrain);
</span><span class="comment">        Xtrain(c,:,:) = double(reshape(mnist.train_images(:,:,ndx), [28*28 length(ndx)]))';
</span><span class="comment">        ytrain(c,:) = c*ones(Ntrain,1);
</span><span class="comment">        ndx = find(mnist.test_labels==c);
</span><span class="comment">        ndx = ndx(1:Ntest);
</span><span class="comment">        Xtest(c,:,:) = double(reshape(mnist.test_images(:,:,ndx), [28*28 length(ndx)]))';
</span><span class="comment">        ytest(c,:) = c*ones(Ntest,1);
</span><span class="comment">      end
</span><span class="comment">      Xtrain = reshape(Xtrain, 10*Ntrain, 784);
</span><span class="comment">      ytrain = ytrain(:);
</span><span class="comment">      Xtest = reshape(Xtest, 10*Ntest, 784);
</span><span class="comment">      ytest = ytest(:);
</span><span class="comment">
</span><span class="comment">      m = fit(logregDist, 'X', Xtrain, 'y', ytrain, 'lambda', 1e-3, 'prior', 'L2');
</span><span class="comment">      pred = predict(m, Xtest);
</span><span class="comment">
</span><span class="comment">    end
</span><span class="comment">    %}
</span>  <span class="keyword">end</span>
</pre><pre class="codeinput"><span class="keyword">end</span>
</pre><p class="footer"><br>
            Published with MATLAB&reg; 7.6<br></p>
      </div>
      <!--
##### SOURCE BEGIN #####
classdef multinomLogregDist < condProbDist
  
  properties
    w; 
    transformer;
    nclasses;
  end
  
  %% Main methods
  methods 
    function m = multinomLogregDist(varargin)
      [transformer,  w, C] = process_options(...
        varargin, 'transformer', [], 'w', [], 'nclasses', []);
      m.transformer = transformer;
      m.w = w;
      m.nclasses = C;
    end
    
   
     function [obj, output] = fit(obj, varargin)
       % model = fit(model, 'name1', val1, 'name2', val2, ...)
       % Arguments are
       % 'X' - X(i,:) Do NOT include a column of 1's
       % 'y'- y(i) in {1,...,K}
       % 'prior' - one of { 'none', 'L2', 'L1'}
       % 'lambda' - >= 0
       % 'method' - one of {'boundoptRelaxed', 'boundoptStepwise', any minFunc method}
       output = [];
       [X, y,  prior, lambda, method] = process_options(...
         varargin, 'X', [], 'y', [],  'prior', 'none', 'lambda', 0, ...
         'method', 'boundopt');
       if lambda > 0 && strcmpi(prior, 'none'), prior = 'L2'; end
       if ~isempty(obj.transformer)
         [X, obj.transformer] = train(obj.transformer, X);
       end
       [n d] = size(X);
       C = obj.nclasses;
       if isempty(C), C = length(unique(y)); end
       Y1 = oneOfK(y, C);
       switch lower(prior)
         case {'l2', 'none'}
           options = optimset('Display','none','Diagnostics','off','GradObj','on',...
             'Hessian','on','DerivativeCheck','off');
           switch lower(method)
             case 'boundoptrelaxed'
               %obj.w = compileAndRun('boundOptL2overrelaxed',X,Y1,lambda); 
               [obj.w, output] = boundOptL2overrelaxed(X, Y1, lambda);
             case 'boundoptstepwise'
                 [obj.w, output] = boundOptL2Stepwise(X, Y1, lambda);
             otherwise
               options.Method = method;
               winit = zeros(d*(C-1),1);
               [obj.w, f, exitflag, output] = minFunc(@multinomLogregNLLgradHessL2, winit, options, X, Y1, lambda);
           end
         case {'l1'}
           switch method
             case 'boundopt'
               obj.w = boundOptL1overrelaxed(X, Y1, lambda);
             otherwise
               error(['unrecognized method ' method])
           end
         otherwise
           error(['unrecognized prior ' prior])
       end
     end
     
      function p = logprob(obj, X, y)
       % p(i) = log p(Y(i) | X(i,:), params), Y(i) in 1...C
       pred = predict(obj, X);
       P = pred.probs;
       Y = oneOfK(y, obj.nclasses);
       p =  sum(sum(Y.*log(P)));
     end
     
     function pred = predict(obj, X, w)
       % X(i,:) is case i
       % pred(i) = DiscreteDist(y|X(i,:))
       if nargin < 3, w = obj.w; end
       if ~isempty(obj.transformer)
         X = test(obj.transformer, X);
       end
       P = multiSigmoid(X,w);
       pred = discreteDist(P);
     end
     
  end
  
  %% Demos
  methods(Static = true)
    
    function test()
      % check functions are syntactically correct
      n = 10; d = 3; C = 4;
      X = randn(n,d );
      y = sampleDiscrete((1/C)*ones(1,C), n, 1);
      m = multinomLogregDist('nclasses', C);
      m = fit(m, 'X', X, 'y', y);
      pred = predict(obj, X);
      ll = logprob(obj, X, y);
    end
    
    function demoCrabs()
      [Xtrain, ytrain, Xtest, ytest] = makeCrabs;
      sigma2 = 32/5;
      T = chainTransformer({standardizeTransformer(false), kernelTransformer('rbf', sigma2)});    
      m = multinomLogregDist('nclasses', 2, 'transformer', T);
      lambda = 1e-3;
      m = fit(m, 'X', Xtrain, 'y', ytrain, 'lambda', lambda, 'method', 'boundopt');
      %m = fit(m, 'X', Xtrain, 'y', ytrain, 'lambda', lambda, 'method', 'newton');
      m.w(:)'
      P = predict(m, Xtest);
      yhat = mode(P); % 1 or 2
      errs = find(yhat(:) ~= ytest(:));
      nerrs = length(errs)
      %figure;plot(P.probs(:,1))
    end

    function demoOptimizer()
      setSeed(1);
      load soy; % n=307, d = 35, C = 3;
      %load car; % n=1728, d = 6, C = 3;
      methods = {'bb',  'cg', 'lbfgs', 'newton',  'boundoptRelaxed'};
      lambda = 1e-3;
      figure; hold on;
      [styles, colors, symbols] =  plotColors;
      for mi=1:length(methods)
        tic
        [m, output{mi}] = fit(multinomLogregDist, 'X', X, 'y', Y, ...
          'lambda', lambda, 'method', methods{mi});
        T = toc
        time(mi) = T;
        w{mi} = m.w;
        niter = length(output{mi}.ftrace)
        h(mi) = plot(linspace(0, T, niter), output{mi}.ftrace, styles{mi});
        legendstr{mi}  = sprintf('%s', methods{mi});
      end
      legend(legendstr)
    end
    
    %{
    function demoMnist()
      load('mnistALL')
      % train_images: [28x28x60000 uint8]
      % test_images: [28x28x10000 uint8]
      % train_labels: [60000x1 uint8]
      % test_labels: [10000x1 uint8]
      setSeed(0);
      Ntrain = 100;
      Ntest = 1000;
      Xtrain = zeros(10, Ntrain, 784);
      ytrain = zeros(10, Ntrain);
      Xtest = zeros(10, Ntrain, 784);
      ytest = zeros(10, Ntest);
      for c=1:10
        ndx = find(mnist.train_labels==c);
        ndx = ndx(1:Ntrain);
        Xtrain(c,:,:) = double(reshape(mnist.train_images(:,:,ndx), [28*28 length(ndx)]))';
        ytrain(c,:) = c*ones(Ntrain,1);
        ndx = find(mnist.test_labels==c);
        ndx = ndx(1:Ntest);
        Xtest(c,:,:) = double(reshape(mnist.test_images(:,:,ndx), [28*28 length(ndx)]))';
        ytest(c,:) = c*ones(Ntest,1);
      end
      Xtrain = reshape(Xtrain, 10*Ntrain, 784);
      ytrain = ytrain(:);
      Xtest = reshape(Xtest, 10*Ntest, 784);
      ytest = ytest(:);
      
      m = fit(logregDist, 'X', Xtrain, 'y', ytrain, 'lambda', 1e-3, 'prior', 'L2');
      pred = predict(m, Xtest);
      
    end
    %}
  end
end
##### SOURCE END #####
-->
   </body>
</html>